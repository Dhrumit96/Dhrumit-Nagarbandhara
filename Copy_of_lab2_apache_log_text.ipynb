{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 0,
      "labels_anchors": false,
      "latex_user_defs": false
    },
    "nav_menu": {
      "height": "559px",
      "width": "420px"
    },
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "Copy of lab2_apache_log_text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vpe0cDYh9pYm",
        "colab_type": "text"
      },
      "source": [
        "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q57K1V-9pYo",
        "colab_type": "text"
      },
      "source": [
        "<p style=\"font-size:20px;font-weight: bold;color:blue\">\"Digital College Paris\" </p>\n",
        "<p style=\"font-size:15px;font-weight: bold;color:blue\">Big Data - MBA IT -- Instructor: B. Men</p>\n",
        "\n",
        "**Sources**: These labs synthetize and *builds on* labs from several origins: \n",
        "- The series of moocs from Berkeley and Databricks,(Creative Commons licences), namely\n",
        "   - [Introduction to Apache Spark](https://courses.edx.org/courses/course-v1:BerkeleyX+CS105x+1T2016/info)\n",
        "   - [Big data Analysis with Apache Spark](https://courses.edx.org/courses/course-v1:BerkeleyX+CS110x+2T2016/info)\n",
        "   - [Distributed Machine Learning with Apache Spark](https://courses.edx.org/courses/course-v1:BerkeleyX+CS120x+2T2016/info)\n",
        "   - [Introduction to Big Data with Apache Spark](https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/info)\n",
        "   - [Scalable Machine Learning](https://courses.edx.org/courses/BerkeleyX/CS190.1x/1T2015/info)\n",
        "- [Apache Spark & Python (pySpark) tutorials for Big Data Analysis and Machine Learning](https://github.com/jadianes/spark-py-notebooks) (Apache License, Version 2.0)\n",
        "\n",
        "We have kept the labs text in english. This will enable us to reuse them in international sections. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUj0u3H09pYq",
        "colab_type": "text"
      },
      "source": [
        "<p style=\"font-size:35px;font-weight: bold;\">Lab 2 : Web Server Log Analysis with Apache Spark</p>\n",
        "\n",
        "This lab will demonstrate how easy it is to perform web server log analysis with Apache Spark.\n",
        "\n",
        " \n",
        "Server log analysis is an ideal use case for Spark.  It's a very large, common data source and contains a rich set of information.  Spark allows you to store your logs in files on disk cheaply, while still providing a quick and simple way to perform data analysis on them.  This homework will show you how to use Apache Spark on real-world text-based production logs and fully harness the power of that data.  Log data comes from many sources, such as web, file, and compute servers, application logs, user-generated content,  and can be used for monitoring servers, improving business and customer intelligence, building recommendation systems, fraud detection, and much more.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT-Bo-jz9pYs",
        "colab_type": "text"
      },
      "source": [
        "<p style=\"font-size:20pt; font-weight: bold;color:blue\"> How to complete this lab :</p>\n",
        "\n",
        " This assignment is broken up into sections with bite-sized examples for demonstrating Spark functionality for log processing. For each problem, you should start by thinking about the algorithm that you will use to *efficiently* process the log in a parallel, distributed manner. This means using the various [RDD](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) operations along with [`lambda` functions](https://docs.python.org/2/tutorial/controlflow.html#lambda-expressions) that are applied at each worker.\n",
        "\n",
        " \n",
        "This assignment consists of 4 parts:\n",
        "\n",
        "- Part 1 : Apache Web Server Log file format\n",
        "- Part 2 : Sample Analyses on the Web Server Log File with Spark Core\n",
        "- Part 3 : Analyzing Web Server Log File with Spark SQL\n",
        "- Part 4 : Exploring 404 Response Codes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vle6OVEd9pYu",
        "colab_type": "text"
      },
      "source": [
        "##  Prerequisites : Spark Context configuration \n",
        "\n",
        "<p>If you can't succeed to create SparContext and SQLContext objects such as explained on your course :<br/>\n",
        "Remove the following comments and modify the \"spark_path\" variable according to your spark location path.\n",
        "Using Colaboratory : Execute this cell, this will install java and spark in the notebook and set all the environment.</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KnBBVm4-pkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtLTiWPU-rcT",
        "colab_type": "code",
        "outputId": "3144d5b8-9daf-4644-e7ca-b79f4269180d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiHlLwOZ9pYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "import sys\n",
        "\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "spark = SparkSession.builder.appName(\"Digital College Paris\").master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUNn8w5P9pY3",
        "colab_type": "text"
      },
      "source": [
        "# Part 1 : Apache Web Server Log file format\n",
        "\n",
        "If you're familiar with web servers at all, you'll recognize that this is in [Common Log Format](https://www.w3.org/Daemon/User/Config/Logging.html#common-logfile-format). The fields are:\n",
        "\n",
        "_remotehost rfc931 authuser [date] \"request\" status bytes_\n",
        "\n",
        "| field         | meaning                                                                |\n",
        "| ------------- | ---------------------------------------------------------------------- |\n",
        "| _remotehost_  | Remote hostname (or IP number if DNS hostname is not available).       |\n",
        "| _rfc931_      | The remote logname of the user. We don't really care about this field. |\n",
        "| _authuser_    | The username of the remote user, as authenticated by the HTTP server.  |\n",
        "| _[date]_      | The date and time of the request.                                      |\n",
        "| _\"request\"_   | The request, exactly as it came from the browser or client.            |\n",
        "| _status_      | The HTTP status code the server sent back to the client.               |\n",
        "| _bytes_       | The number of bytes (`Content-Length`) transferred to the client.      |\n",
        "\n",
        "\n",
        "\n",
        "The log file entries produced in CLF will look something like this:\n",
        "\n",
        "`127.0.0.1 - - [01/Aug/1995:00:00:01 -0400] \"GET /images/launch-logo.gif HTTP/1.0\" 200 1839`\n",
        " \n",
        "Each part of this log entry is described below.\n",
        "\n",
        "* `127.0.0.1`\n",
        "This is the IP address (or host name, if available) of the client (remote host) which made the request to the server.\n",
        "\n",
        " \n",
        "* `-`\n",
        "The \"hyphen\" in the output indicates that the requested piece of information (user identity from remote machine) is not available.\n",
        "\n",
        " \n",
        "* `-`\n",
        "The \"hyphen\" in the output indicates that the requested piece of information (user identity from local logon) is not available.\n",
        "\n",
        " \n",
        "* `[01/Aug/1995:00:00:01 -0400]`\n",
        "The time that the server finished processing the request. The format is:\n",
        "\n",
        "`[day/month/year:hour:minute:second timezone]`\n",
        "  * day = 2 digits\n",
        "  * month = 3 letters\n",
        "  * year = 4 digits\n",
        "  * hour = 2 digits\n",
        "  * minute = 2 digits\n",
        "  * second = 2 digits\n",
        "  * zone = (\\+ | \\-) 4 digits\n",
        " \n",
        "* `\"GET /images/launch-logo.gif HTTP/1.0\"`\n",
        "This is the first line of the request string from the client. It consists of a three components: the request method (e.g., `GET`, `POST`, etc.), the endpoint (a [Uniform Resource Identifier](http://en.wikipedia.org/wiki/Uniform_resource_identifier)), and the client protocol version.\n",
        "\n",
        " \n",
        "* `200`\n",
        "This is the status code that the server sends back to the client. This information is very valuable, because it reveals whether the request resulted in a successful response (codes beginning in 2), a redirection (codes beginning in 3), an error caused by the client (codes beginning in 4), or an error in the server (codes beginning in 5). The full list of possible status codes can be found in the HTTP specification ([RFC 2616](https://www.ietf.org/rfc/rfc2616.txt) section 10).\n",
        "\n",
        " \n",
        "* `1839`\n",
        "The last entry indicates the size of the object returned to the client, not including the response headers. If no content was returned to the client, this value will be \"-\" (or sometimes 0).\n",
        "\n",
        " \n",
        "Note that log files contain information supplied directly by the client, without escaping. Therefore, it is possible for malicious clients to insert control-characters in the log files, *so care must be taken in dealing with raw logs.*\n",
        "\n",
        " \n",
        "## NASA-HTTP Web Server Log\n",
        "For this assignment, we will use a data set from NASA Kennedy Space Center WWW server in Florida. The full data set is freely available (http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html) and contains two month's of all HTTP requests. We are using a subset that only contains several days worth of requests.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtXEpjqB9pY5",
        "colab_type": "text"
      },
      "source": [
        "## 1. Parsing Each Log Line\n",
        "Using the CLF as defined above, we create a regular expression pattern to extract the nine fields of the log line using the Python regular expression [`search` function](https://docs.python.org/2/library/re.html#regular-expression-objects). The function returns a pair consisting of a Row object and 1. If the log line fails to match the regular expression, the function returns a pair consisting of the log line string and 0. A '-' value in the content size field is cleaned up by substituting it with 0. The function converts the log line's date string into a Python `datetime` object using the given `parse_apache_time` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6aUy3ur9pY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import datetime\n",
        "\n",
        "from pyspark.sql import Row\n",
        "\n",
        "month_map = {'Jan': 1, 'Feb': 2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7,\n",
        "    'Aug':8,  'Sep': 9, 'Oct':10, 'Nov': 11, 'Dec': 12}\n",
        "\n",
        "def parse_apache_time(s):\n",
        "    \"\"\" Convert Apache time format into a Python datetime object\n",
        "    Args:\n",
        "        s (str): date and time in Apache time format\n",
        "    Returns:\n",
        "        datetime: datetime object (ignore timezone for now)\n",
        "    \"\"\"\n",
        "    return datetime.datetime(int(s[7:11]),\n",
        "                             month_map[s[3:6]],\n",
        "                             int(s[0:2]),\n",
        "                             int(s[12:14]),\n",
        "                             int(s[15:17]),\n",
        "                             int(s[18:20]))\n",
        "\n",
        "\n",
        "def parseApacheLogLine(logline):\n",
        "    \"\"\" Parse a line in the Apache Common Log format\n",
        "    Args:\n",
        "        logline (str): a line of text in the Apache Common Log format\n",
        "    Returns:\n",
        "        tuple: either a dictionary containing the parts of the Apache Access Log and 1,\n",
        "               or the original invalid log line and 0\n",
        "    \"\"\"\n",
        "    match = re.search(APACHE_ACCESS_LOG_PATTERN, logline)\n",
        "    if match is None:\n",
        "        return (logline, 0)\n",
        "    size_field = match.group(9)\n",
        "    if size_field == '-':\n",
        "        size = 0\n",
        "    else:\n",
        "        size = match.group(9)\n",
        "    return (Row(\n",
        "        host          = match.group(1),\n",
        "        client_identd = match.group(2),\n",
        "        user_id       = match.group(3),\n",
        "        date_time     = parse_apache_time(match.group(4)),\n",
        "        method        = match.group(5),\n",
        "        endpoint      = match.group(6),\n",
        "        protocol      = match.group(7),\n",
        "        response_code = int(match.group(8)),\n",
        "        content_size  = int(size)\n",
        "    ), 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SYuaTQC9pY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A regular expression pattern to extract fields from the log line\n",
        "APACHE_ACCESS_LOG_PATTERN = '^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+)\\s*(\\S*)\\s*\" (\\d{3}) (\\S+)'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jkV8x9m9pZG",
        "colab_type": "text"
      },
      "source": [
        "## 2. Configuration and Initial RDD Creation\n",
        "We are ready to specify the input log file and create an RDD containing the parsed log file data. The log file has already been downloaded for you.\n",
        "\n",
        " \n",
        "To create the primary RDD that we'll use in the rest of this assignment, we first load the text file using [`sc.textfile(logFile)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.textFile) to convert each line of the file into an element in an RDD.\n",
        "\n",
        "Next, we use [`map(parseApacheLogLine)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) to apply the parse function to each element (that is, a line from the log file) in the RDD and turn each line into a pair [`Row` object](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row).\n",
        "\n",
        "Finally, we cache the RDD in memory since we'll use it throughout this notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCjaNLL39pZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "from databricks_test_helper import Test\n",
        "\n",
        "baseDir = os.path.join('..')\n",
        "inputPath = os.path.join('/content/drive/My Drive/Colab Notebooks/data', 'apache.access.log.PROJECT')\n",
        "logFile = os.path.join(baseDir, inputPath)\n",
        "\n",
        "def parseLogs():\n",
        "    \"\"\" Read and parse log file \"\"\"\n",
        "    parsed_logs = (sc\n",
        "                   .textFile(logFile)\n",
        "                   .map(parseApacheLogLine)\n",
        "                   .cache())\n",
        "\n",
        "    access_logs = (parsed_logs\n",
        "                   .filter(lambda s: s[1] == 1)\n",
        "                   .map(lambda s: s[0])\n",
        "                   .cache())\n",
        "\n",
        "    failed_logs = (parsed_logs\n",
        "                   .filter(lambda s: s[1] == 0)\n",
        "                   .map(lambda s: s[0]))\n",
        "    failed_logs_count = failed_logs.count()\n",
        "    if failed_logs_count > 0:\n",
        "        print('Number of invalid logline: %d' % failed_logs.count())\n",
        "        for line in failed_logs.take(20):\n",
        "            print('Invalid logline: %s' % line)\n",
        "\n",
        "    print('Read %d lines, successfully parsed %d lines, failed to parse %d lines' % (parsed_logs.count(), access_logs.count(), failed_logs.count()))\n",
        "    return parsed_logs, access_logs, failed_logs\n",
        "\n",
        "\n",
        "parsed_logs, access_logs, failed_logs = parseLogs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgzcaajz9pZL",
        "colab_type": "text"
      },
      "source": [
        "# Part 2 : Sample Analyses on the Web Server Log File with Spark Core\n",
        " \n",
        "Now that we have an RDD containing the log file as a set of Row objects, we can perform various analyses.\n",
        "\n",
        " \n",
        "## 1. Content Size Statistics\n",
        "\n",
        " \n",
        "Let's compute some statistics about the sizes of content being returned by the web server. In particular, we'd like to know what are the average, minimum, and maximum content sizes.\n",
        "\n",
        " \n",
        "We can compute the statistics by applying a `map` to the `access_logs` RDD. The `lambda` function we want for the map is to extract the `content_size` field from the RDD. The map produces a new RDD containing only the `content_sizes` (one element for each Row object in the `access_logs` RDD). To compute the minimum and maximum statistics, we can use [`min()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.min) and [`max()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.max) functions on the new RDD. We can compute the average statistic by using the [`reduce`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduce) function with a `lambda` function that sums the two inputs, which represent two elements from the new RDD that are being reduced together. The result of the `reduce()` is the total content size from the log and it is to be divided by the number of requests as determined using the [`count()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.count) function on the new RDD.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQCjFbwM9pZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "# Calculate statistics based on the content size\n",
        "# HINT : RDD has been indexed in function parseApacheLogLine(). \n",
        "# You can now access content sizes using attribute .content_size\n",
        "content_sizes = access_logs.map(lambda log: log.content_size).cache()\n",
        "content_sizes_mean= content_sizes.reduce(lambda a, b : a + b) / content_sizes.count()\n",
        "content_sizes_min = content_sizes.min()\n",
        "content_sizes_max = content_sizes.max()\n",
        "print('Content Size Avg: %i, Min: %i, Max: %s' % (round(content_sizes_mean), content_sizes_min, content_sizes_max))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzSvORhq9pZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST Content size statistics\n",
        "Test.assertEquals((round(content_sizes_mean), content_sizes_min, content_sizes_max), (17532, 0, 3421948), 'incorrect expected values')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTbWLXGN9pZU",
        "colab_type": "text"
      },
      "source": [
        "## 2. Response Code Analysis\n",
        "\n",
        "Next, lets make a count of the response codes that appear in the logs. As with the content size analysis, first we create a new RDD by using a `lambda` function to extract the `response_code` field from the `access_logs` RDD. The difference here is that we will use a [pair tuple](https://docs.python.org/2/tutorial/datastructures.html?highlight=tuple#tuples-and-sequences) instead of just the field itself. Using a pair tuple consisting of the response code and 1 will let us count how many records have a particular response code. Using the new RDD, we perform a [`reduceByKey`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) function. `reduceByKey` performs a reduce on a per-key basis by applying the `lambda` function to each element, pairwise with the same key. We use the simple `lambda` function of adding the two values. Then, we cache the resulting RDD and create a list by using the [`take`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take) function.\n",
        "\n",
        "_Note_ : The expected method is similar of the word count approach developed in last week TP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWnKcH9I9pZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "# Make a Response Code repartition count.\n",
        "# HINT : you can access to the log response_code using attribute \".response_code\".\n",
        "\n",
        "responseCodeToCount = (access_logs\n",
        "                       .map(lambda log: (log.response_code, 1))\n",
        "                       .reduceByKey(lambda a, b : a + b)\n",
        "                       .cache())\n",
        "responseCodeToCountList = responseCodeToCount.collect()\n",
        "print('Found %d response codes' % len(responseCodeToCountList))\n",
        "print('Response Code Counts: %s' % responseCodeToCountList)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ldlwV3K9pZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST : Response Code Analysis\n",
        "Test.assertEquals(sorted(responseCodeToCountList), [(200, 940847), (302, 16244), (304, 79824), (403, 58), (404, 6185), (500, 2), (501, 17)], \"Incorrect response code analysis\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgshZOo09pZd",
        "colab_type": "text"
      },
      "source": [
        "## 3. Response Code Graphing with `matplotlib`\n",
        "Now, lets visualize the results from the last example. We can visualize the results from the last example using [`matplotlib`](http://matplotlib.org/). First we need to extract the labels and fractions for the graph. We do this with two separate `map` functions with a `lambda` functions. The first `map` function extracts a list of of the response code values, and the second `map` function extracts a list of the per response code counts  divided by the total size of the access logs. Next, we create a figure with `figure()` constructor and use the `pie()` method to create the pie plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmWrJ-TX9pZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "# Assign the response code labels using .map() and .collect() to \"labels\" variable.\n",
        "# Assign the number of rows to variable \"rows_count\" (don't forget to collect).\n",
        "# Assign the proportional count to variable \"fracs_count\" (don't forget to collect).\n",
        "labels = responseCodeToCount.map(lambda x: x[0]).collect()\n",
        "count = access_logs.count()\n",
        "fracs_count = responseCodeToCount.map(lambda x: (float(x[1]) / count)).collect()\n",
        "print('labels')\n",
        "print('fracs_count')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8IiBbFa9pZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST : Response code graphing\n",
        "Test.assertEquals(sorted(fracs_count), [1.9172201841106543e-06, 1.629637156494056e-05, 5.5599385339208974e-05, 0.005929003419362198, 0.015571662335346735, 0.07652009198822443, 0.9019054292799784], \"Incorrect proportions\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOmPDIcK9pZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def pie_pct_format(value):\n",
        "    \"\"\" Determine the appropriate format string for the pie chart percentage label\n",
        "    Args:\n",
        "        value: value of the pie slice\n",
        "    Returns:\n",
        "        str: formated string label; if the slice is too small to fit, returns an empty string for label\n",
        "    \"\"\"\n",
        "    return '' if value < 7 else '%.0f%%' % value\n",
        "\n",
        "fig = plt.figure(figsize=(4.5, 4.5), facecolor='white', edgecolor='white')\n",
        "colors = ['yellowgreen', 'lightskyblue', 'gold', 'purple', 'lightcoral', 'yellow', 'black']\n",
        "explode = (0.05, 0.05, 0.1, 0, 0, 0, 0)\n",
        "patches, texts, autotexts = plt.pie(fracs_count, labels=labels, colors=colors,\n",
        "                                    explode=explode, autopct=pie_pct_format,\n",
        "                                    shadow=False,  startangle=125)\n",
        "for text, autotext in zip(texts, autotexts):\n",
        "    if autotext.get_text() == '':\n",
        "        text.set_text('')  # If the slice is small to fit, don't show a text label\n",
        "plt.legend(labels, loc=(0.80, -0.1), shadow=True)\n",
        "pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUjU5jUN9pZr",
        "colab_type": "text"
      },
      "source": [
        "## 4. Visualizing Endpoints\n",
        "\n",
        "Now, lets visualize the number of hits to endpoints (URIs) in the log. To do that, perform a endpoints counts similar to response codes approach.\n",
        "\n",
        "_Hint_ : To perform this task, we first create a new RDD by using a `lambda` function to extract the `endpoint` field from the `access_logs` RDD using a pair tuple consisting of the endpoint and 1 which will let us count how many records were created by a particular host's request. Using the new RDD, we perform a `reduceByKey` function with a `lambda` function that adds the two values. We then cache the results.\n",
        "\n",
        " \n",
        "Next we visualize the results using `matplotlib`. We previously imported the `matplotlib.pyplot` library, so we do not need to import it again. We perform two separate `map` functions with `lambda` functions. The first `map` function extracts a list of endpoint values, and the second `map` function extracts a list of the visits per endpoint values. Next, we create a figure with `figure()` constructor, set various features of the plot (axis limits, grid lines, and labels), and use the `plot()` method to create the line plot.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCH9NPZe9pZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "# Assign the endpoints <key, value> counts to variable \"endpoints\" using .map() and .reduceByKey().\n",
        "# Assign the endpoints labels to variable \"ends_labels\" (don't forget to collect).\n",
        "# Assign the count to variables \"end_counts\" (don't forget to collect).\n",
        "endpoints = (access_logs\n",
        "             .map(lambda log: (log.endpoint, 1))\n",
        "             .reduceByKey(lambda a, b : a + b)\n",
        "             .cache())\n",
        "ends_labels = endpoints.map(lambda x: x[0]).collect()\n",
        "ends_counts = endpoints.map(lambda x :x[1]).collect()\n",
        "\n",
        "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
        "plt.axis([0, len(ends_labels), 0, max(ends_counts)])\n",
        "plt.grid(b=True, which='major', axis='y')\n",
        "plt.xlabel('Endpoints')\n",
        "plt.ylabel('Number of Hits')\n",
        "plt.plot(ends_counts)\n",
        "pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VD1syxo9pZu",
        "colab_type": "text"
      },
      "source": [
        "## 5. Top 10 transferred bytes hosts\n",
        "`\n",
        "Now, let's answer the following question. Who are the top 10 hosts in terms of transferred bytes (content size) ? \n",
        "\n",
        "To perform this task, use RDD transformations `map`, `reduceByKey` and [`takeOrdered`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.takeOrdered)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9tkc-PU9pZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "# Assign the top 10 hosts considering transfered bytes to the variable \"top_10_hosts\".\n",
        "#endpoints.map(lambda x: x[0])\n",
        "\n",
        "# hostCountPairTuple = access_logs.map(lambda log: (log.content_size))\n",
        "\n",
        "# hostSum = hostCountPairTuple.reduceByKey(lambda a, b : a + b)\n",
        "\n",
        "# hostMoreThan10 = hostSum.filter(lambda s: s[1] > 0)\n",
        "\n",
        "# top_10_hosts = (hostMoreThan10\n",
        "#                .map(lambda s: s[0])\n",
        "#                .takeOrdered(10))\n",
        "# print(top_10_hosts)\n",
        "\n",
        "\n",
        "host_size_tuple = (access_logs\n",
        "                  .map(lambda log: (log.host, log.content_size))\n",
        "                  .reduceByKey(lambda a, b : a + b)\n",
        "                  .takeOrdered(10,lambda s: -1 * s[1]))\n",
        "\n",
        "top_10_hosts = []\n",
        "for host,_ in host_size_tuple:\n",
        "  top_10_hosts.append(host)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y-v6bwNen2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST : Top 10 tranfered bytes hosts\n",
        "Test.assertEquals(len(top_10_hosts), 10, \"top_10_hosts had to be length 10\")\n",
        "Test.assertEquals(top_10_hosts, ['news.ti.com', 'www-relay.pa-x.dec.com', 'piweba5y.prodigy.com', 'e659229.boeing.com', 'piweba3y.prodigy.com', 'www-c2.proxy.aol.com', '163.206.89.4', 'www-b3.proxy.aol.com', 'webgate1.mot.com', 'gatekeeper.cca.rockwell.com'], \"Incorrect top 10 hosts\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23-100eq9pZ2",
        "colab_type": "text"
      },
      "source": [
        "# Part 3 : Analyzing Web Server Log File with Spark SQL\n",
        " \n",
        "Now it is time to perform advanced analytics on web server log files using Spark SQL.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlDwJeL69pZ3",
        "colab_type": "text"
      },
      "source": [
        "## 1. Transform Spark RDD to Spark SQL dataframe\n",
        "\n",
        "In order to use Spark SQL functionalities, you need to transform your logs data spark RDD to a Spark SQL dataframe. To perform this task, you can refer to the Spark lectures provided in classroom or spark documentation. After dataframe load, make sure that dataframe column casting is correct using method `printSchema`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-wv50hR9pZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "# Transform the access_logs spark RDD into Spark SQL Dataframe.\n",
        "# Hint : Use the sqlContext and the .createDataFrame() method.\n",
        "logs_df = sqlContext.createDataFrame(access_logs)\n",
        "logs_df = logs_df.cache()\n",
        "logs_df.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7diamaSh9pZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST : Transform Spark RDD to Spark SQL dataframe\n",
        "Test.assertEquals(str(type(logs_df)), \"<class 'pyspark.sql.dataframe.DataFrame'>\", \"logs_df is not a Spark DataFrame.\")\n",
        "Test.assertEquals(logs_df.dtypes, [('client_identd', 'string'), ('content_size', 'bigint'), ('date_time', 'timestamp'), ('endpoint', 'string'), ('host', 'string'), ('method', 'string'), ('protocol', 'string'), ('response_code', 'bigint'), ('user_id', 'string')], \"Dataframe casting is not correct.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmMrCksU9pZ9",
        "colab_type": "text"
      },
      "source": [
        "## 2. Top 10 error endpoints\n",
        "\n",
        "What are the top twenty paths which did not have return code 200? Create a sorted list containing the paths and the number of times that they were accessed with a non-200 return code and show the top ten.\n",
        "\n",
        "Think about the steps that you need to perform to determine which paths did not have a 200 return code, how you will uniquely count those paths and sort the list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRBDbpsz9paA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "# Assign logs with status different than 200 to variable \"not200_df\".\n",
        "# Assign the counts per endpoint dataframe to variable \"logs_sums_df\". \n",
        "# Descending sort the logs_sum_df and assign the result to variable \"sorted_logs_sum_df\".\n",
        "# Collect the top 10 endpoints and assign it to variable \"top_ten_err_urls\".\n",
        "# Hint : You will need to use methods .groupBy() and .sort() to achieve this task.\n",
        "# Note : You are welcome to structure your solution in a different way, so long as\n",
        "# you ensure the variables used in the next Test section are defined (ie. logs_sum_df, top_ten_err_urls).\n",
        "\n",
        "not200_df = logs_df.filter(\"response_code != 200\")\n",
        "logs_sum_df = not200_df.groupBy(\"endpoint\").count()\n",
        "sorted_logs_sum_df = logs_sum_df.sort(\"count\",ascending=False)\n",
        "top_ten_err_urls = sorted_logs_sum_df.take(10)\n",
        "\n",
        "# Display\n",
        "print('Top Ten failed URLs:')\n",
        "sorted_logs_sum_df.show(10, False)\n",
        "print(top_ten_err_urls)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC_gMspi9paD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST Top ten error endpoints\n",
        "Test.assertEquals(logs_sum_df.count(), 7689, 'incorrect count for endpointSum')\n",
        "Test.assertEquals(top_ten_err_urls, [(u'/images/NASA-logosmall.gif', 8761), (u'/images/KSC-logosmall.gif', 7236), (u'/images/MOSAIC-logosmall.gif', 5197), (u'/images/USA-logosmall.gif', 5157), (u'/images/WORLD-logosmall.gif', 5020), (u'/images/ksclogo-medium.gif', 4728), (u'/history/apollo/images/apollo-logo1.gif', 2907), (u'/images/launch-logo.gif', 2811), (u'/', 2199), (u'/images/ksclogosmall.gif', 1622)], 'incorrect Top Ten failed URLs (topTenErrURLs)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P3oiRkp9paG",
        "colab_type": "text"
      },
      "source": [
        "## 3. Number of Unique Hosts\n",
        "\n",
        "How many unique hosts are there in the entire log?\n",
        "\n",
        " \n",
        "Think about the steps that you need to perform to count the number of different hosts in the log.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI8SS2qZ9paG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "# Note : There are several ways to achieve this task.\n",
        "unique_host_count = logs_df.select('host').distinct().count()\n",
        "print('Unique hosts: {0}'.format(unique_host_count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAlVo5fl9paJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST Number of unique hosts\n",
        "Test.assertEquals(unique_host_count, 54507, 'incorrect unique_host_count')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ld1aXds9paM",
        "colab_type": "text"
      },
      "source": [
        "## 4. Extract the date day using an UDF\n",
        "\n",
        "In the next questions, we will compute the number of unique Daily hosts given the day of the month. To perform this following task, we need to create a new column which contains the day of the month (from 01 to 31). Fortunately, the day of the month is contained in the column `date_time` pattern.\n",
        "\n",
        "_Note_ : There are several ways to achieve this task including using method pyspark sql function `dayofmonth`. In this exercise, we will use a UDF based solution in order to make you practice with the UDF concept. It is strongly recommended for you to look at the UDF slides provided during lecture 3.\n",
        "\n",
        "_Note_ : Since the log only covers a single month, you can ignore the month."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuBHNLIe9paN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "# Complete the function 'date_to_day' in order to return the day of the month.\n",
        "# Wrap the function into and UDF object. Assign the result to variable \"my_udf\".\n",
        "# Apply the UDF on logs_df using method .WithColumn(). The resulting column has to be named \"day\". Assign the resulting\n",
        "# dataframe to variable \"logs_df_with_day\".\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "def date_to_day(date):\n",
        "    \"\"\"\n",
        "    Extracts the day of month in variable date.\n",
        "    date : timestamp with pattern \"yyyy-mm-dd hh:mm:ss\"\n",
        "    returns : day of the month (pattern 'dd')\n",
        "    \"\"\"\n",
        "    str_date = str(date)\n",
        "    s =  re.search(\"\\d\\d\\d\\d-\\d\\d-(\\d\\d) \\d\\d:\\d\\d:\\d\\d\",str_date)\n",
        "    return s.group(1)\n",
        "\n",
        "# testing the function\n",
        "Test.assertEquals(date_to_day(\"1995-08-01 00:00:07\"), \"01\", 'function date_to_day is not correct.')\n",
        "Test.assertEquals(date_to_day(\"2017-09-17 10:00:07\"), \"17\", 'function date_to_day is not correct.')\n",
        "\n",
        "# Wrap in a UDF and apply on logs_df\n",
        "my_udf = udf(date_to_day)\n",
        "logs_df_with_day = logs_df.withColumn(\"day\",my_udf(logs_df[\"date_time\"]))\n",
        "logs_df_with_day.show(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUN__73t9paR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST Extract the month day using an UDF\n",
        "distinct_days = logs_df_with_day.select(\"day\").distinct().collect()\n",
        "Test.assertEquals(len(distinct_days), 21, 'it seems that UDF is misapplied')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL0Ymvv79paU",
        "colab_type": "text"
      },
      "source": [
        "## 5. Number of Unique Daily Hosts\n",
        "\n",
        "For an advanced exercise, let's determine the number of unique hosts in the entire log on a day-by-day basis. This computation will give us counts of the number of unique daily hosts. We'd like a DataFrame sorted by increasing day of the month which includes the day of the month and the associated number of unique hosts for that day. Make sure you cache the resulting DataFrame `daily_hosts_df` so that we can reuse it in the next exercise.\n",
        "\n",
        "Think about the steps that you need to perform to count the number of different hosts that make requests *each* day.\n",
        "*Since the log only covers a single month, you can ignore the month.*  You may want to use the `day` column you have computed in the previous task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKmig2c99paV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "# Select columns \"day\" and \"host\" from \"logs_df_with_day\" dataframe and assign the result to \"day_to_host_pair_df\".\n",
        "# Remove duplicates <\"day\", \"host\"> duplicates using method .distinct(). Assign the result to day_group_hosts_df.\n",
        "# Group by day and count the distinct hosts. Assign the result dataframe to \"daily_hosts_df\".\n",
        "# Sort per day and assign the result to \"daily_hosts_df_sorted\".\n",
        "# Collect the result to driver. Assign the result to variable \"daily_hosts_list\".\n",
        "\n",
        "\n",
        "day_to_host_pair_df = logs_df_with_day.select(logs_df_with_day[\"host\"],logs_df_with_day[\"day\"] )\n",
        "day_group_hosts_df = (day_to_host_pair_df\n",
        "                      .dropDuplicates())\n",
        "daily_hosts_df = (day_group_hosts_df\n",
        "                 .groupBy('day')\n",
        "                  .count().alias('count')\n",
        "                  .sort(\"day\")\n",
        "                  .cache())\n",
        "daily_hosts_df_sorted = daily_hosts_df.sort(\"count\")\n",
        "daily_hosts_list = (daily_hosts_df\n",
        "                    .rdd\n",
        "                    .map(lambda r: (r[0], r[1]))\n",
        "                    .take(30))\n",
        "\n",
        "print('Unique hosts per day:')\n",
        "daily_hosts_df_sorted.show(30, False)\n",
        "daily_hosts_df = daily_hosts_df.cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzXKIS8L9paY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST Number of unique daily hosts (3c)\n",
        "Test.assertEquals(daily_hosts_df.count(), 21, 'incorrect dailyHosts.count()')\n",
        "Test.assertEquals(daily_hosts_list, [(\"01\", 2582), (\"03\", 3222), (\"04\", 4190), (\"05\", 2502), (\"06\", 2537), (\"07\", 4106), (\"08\", 4406), (\"09\", 4317), (\"10\", 4523), (\"11\", 4346), (\"12\", 2864), (\"13\", 2650), (\"14\", 4454), (\"15\", 4214), (\"16\", 4340), (\"17\", 4385), (\"18\", 4168), (\"19\", 2550), (\"20\", 2560), (\"21\", 4134), (\"22\", 4456)], 'incorrect dailyHostsList')\n",
        "Test.assertTrue(daily_hosts_df.is_cached, 'incorrect dailyHosts.is_cached')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlDUUljd9paa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plotting your results : \n",
        "days = [int(element[0]) for element in daily_hosts_list]\n",
        "hosts = [int(element[1]) for element in daily_hosts_list]\n",
        "plt.plot(days, hosts)\n",
        "plt.axis([0, max(days), 0, max(hosts)+500])\n",
        "plt.xlabel('Day')\n",
        "plt.ylabel('Hosts')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYJQCHTu9pad",
        "colab_type": "text"
      },
      "source": [
        "## 6. Mean Transfered Bytes per status code categories\n",
        "\n",
        "For an advanced exercise, let's determine the mean transfered bytes per status code category. Remember that :\n",
        "- Code beginning with a `2` means a request resulted in a successful response.\n",
        "- Code beginning with a `3` means a request resulted in a redirection.\n",
        "- Code beginning with a `4` means a request resulted in a client error.\n",
        "- Code beginning with a `5` means a request resulted in a server error.\n",
        "\n",
        "For every of these four category, compute the mean transfered bytes (content size).\n",
        "\n",
        "_Hint_ : There are many ways to compute the code category column including the use of an UDF. You can choose the method you prefer. Feel free to refer to spark documentation and StackOverflow posts in order to find functions or informations you are searching for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3BV4HEg9pad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "# Add a new colummn to logs_df contaning the code category. Assign the result to 'logs_df_with_code_category'.\n",
        "# Group the logs per 'code category' computed in first subtask. Assign the result to 'logs_df_groupby_code_category'.\n",
        "# Compute the content size mean per category and return the result to the driver. Assign the result \n",
        "# to 'content_size_per_code_category'.\n",
        "def status_category(status_code):\n",
        "  if str(status_code)[0] == '2':\n",
        "    return 2\n",
        "  if str(status_code)[0] == '3':\n",
        "    return 3\n",
        "  if str(status_code)[0] == '4':\n",
        "    return 4\n",
        "  if str(status_code)[0] == '5':\n",
        "    return 5\n",
        "\n",
        "status_category_udf = udf(status_category)\n",
        "\n",
        "logs_df_with_code_category = logs_df.withColumn('status_category',status_category_udf(logs_df[\"response_code\"]))\n",
        "logs_df_groupby_code_category = logs_df_with_code_category.groupBy(logs_df_with_code_category[\"status_category\"])\n",
        "logs_df_agg_content_size = logs_df_groupby_code_category.agg({\"content_size\":\"avg\"})\n",
        "content_size_per_code_category = []\n",
        "for row in logs_df_agg_content_size.collect():\n",
        "  content_size_per_code_category.append((int(row[\"status_category\"]),row[\"avg(content_size)\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dvkizHT9paf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "round_result = [(el[0], round(el[1], 1)) for el in content_size_per_code_category]\n",
        "Test.assertEquals(len(content_size_per_code_category), 4, 'error : length has to be 4.')\n",
        "Test.assertEquals(round_result, [(3, 14.4), (5, 10.4), (4, 0.0), (2, 19436.9)], 'incorrect mean size content per category.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_c-HE8A9paj",
        "colab_type": "text"
      },
      "source": [
        "# Part 4 : Exploring 404 Response Codes\n",
        " \n",
        "Let's drill down and explore the error 404 response code records. 404 errors are returned when an endpoint is not found by the server (i.e., a missing page or object). During this part, you are free to complete the tasks using `Spark Core` and `access_logs` RDD or using `Spark SQL` and `logs-df` dataframe.\n",
        "\n",
        "_Note_ : Do not forget to cache your RDD / Dataframe in memory in order to reduce computing time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHlSU4IN9pal",
        "colab_type": "text"
      },
      "source": [
        "## 1. Counting 404 Response Codes\n",
        " \n",
        "How many 404 records are there in the logs? Assign the result to variable `badRecords`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxbz3Rxb9pal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "badRecords = logs_df.filter(\"response_code == 404\")\n",
        "badRecords_count = badRecords.count()\n",
        "print('Found %d 404 URLs' % badRecords_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE_bMd439pam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST Counting 404 (4a)\n",
        "Test.assertEquals(badRecords_count, 6185, 'incorrect badRecords.count()')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4PI_yeO9pap",
        "colab_type": "text"
      },
      "source": [
        "## 2. Listing The top-15 404 Response Code endpoints\n",
        "\n",
        "Get the top 15 endpoints that return 404 errors. Assign the result to variable `top_15_404`.\n",
        "\n",
        "_Note_ : variable `top_15_404` has to be a list only containing top 404 error endpoints.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3EnLSSz9paq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "top_15_404_df = badRecords.groupBy(\"endpoint\").count().sort(\"count\",ascending=False).take(15)\n",
        "top_15_404 = []\n",
        "for row in top_15_404_df:\n",
        "  top_15_404.append(row[\"endpoint\"])\n",
        "print('404 Top 15 URLS: %s' % top_15_404)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2un2B1NS9pat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST Listing 404 records (4.2)\n",
        "Test.assertEquals(len(top_15_404), 15, \"top_15_404 length has to be 15.\")\n",
        "Test.assertEquals(top_15_404, ['/pub/winvn/readme.txt', '/pub/winvn/release.txt', '/shuttle/missions/STS-69/mission-STS-69.html', '/images/nasa-logo.gif', '/elv/DELTA/uncons.htm', '/shuttle/missions/sts-68/ksc-upclose.gif', '/history/apollo/sa-1/sa-1-patch-small.gif', '/images/crawlerway-logo.gif', '/://spacelink.msfc.nasa.gov', '/history/apollo/pad-abort-test-1/pad-abort-test-1-patch-small.gif', '/history/apollo/a-001/a-001-patch-small.gif', '/images/Nasa-logo.gif', '/shuttle/resources/orbiters/atlantis.gif', '/history/apollo/images/little-joe.jpg', '/images/lf-logo.gif'], 'top_15_404 not correct')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CY616D79pav",
        "colab_type": "text"
      },
      "source": [
        "## 3. Listing the Top Twenty-five 404 Response Code Hosts\n",
        "\n",
        "Instead of looking at the endpoints that generated 404 errors, let's look at the hosts that encountered 404 errors. Using the RDD / Dataframe containing only log records with a 404 response code that you cached in part (4.1), print out a list of the top twenty-five hosts that generate the most 404 errors. Assign the result to variable `errHostsTop25` and return your results to the driver. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcZ2IR9q9paw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "errHostsTop25_df = badRecords.groupBy(\"host\").count().sort(\"count\",ascending=False).take(25)\n",
        "errHostsTop25 = []\n",
        "for row in errHostsTop25_df:\n",
        "  errHostsTop25.append((row[\"host\"],row[\"count\"]))\n",
        "\n",
        "print('Top 25 hosts that generated errors: %s' % errHostsTop25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "yVnz-MLr9pay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST Top twenty-five 404 response code hosts (4.3)\n",
        "Test.assertEquals(len(errHostsTop25), 25, 'length of errHostsTop25 is not 25')\n",
        "Test.assertEquals(len(set(errHostsTop25) - set([(u'maz3.maz.net', 39), (u'piweba3y.prodigy.com', 39), (u'gate.barr.com', 38), (u'm38-370-9.mit.edu', 37), (u'ts8-1.westwood.ts.ucla.edu', 37), (u'nexus.mlckew.edu.au', 37), (u'204.62.245.32', 33), (u'163.206.104.34', 27), (u'spica.sci.isas.ac.jp', 27), (u'www-d4.proxy.aol.com', 26), (u'www-c4.proxy.aol.com', 25), (u'203.13.168.24', 25), (u'203.13.168.17', 25), (u'internet-gw.watson.ibm.com', 24), (u'scooter.pa-x.dec.com', 23), (u'crl5.crl.com', 23), (u'piweba5y.prodigy.com', 23), (u'onramp2-9.onr.com', 22), (u'slip145-189.ut.nl.ibm.net', 22), (u'198.40.25.102.sap2.artic.edu', 21), (u'gn2.getnet.com', 20), (u'msp1-16.nas.mr.net', 20), (u'isou24.vilspa.esa.es', 19), (u'dial055.mbnet.mb.ca', 19), (u'tigger.nashscene.com', 19)])), 0, 'incorrect errHostsTop25')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtrgL8_59pa0",
        "colab_type": "text"
      },
      "source": [
        "## 4. Listing 404 Response Codes per Day\n",
        "\n",
        "Let's explore the 404 records temporally. Break down the 404 requests by day (`cache()` the RDD `errDateSorted`) and get the daily counts sorted by day as a list. Assign the result to variable `errByDate` and return the result to driver.\n",
        "\n",
        "*Since the log only covers a single month, you can ignore the month in your checks.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3L9WuBU9pa1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "errByDate_df = badRecords.withColumn(\"day\",my_udf(badRecords[\"date_time\"])).groupBy(\"day\").count().sort(\"day\").collect()\n",
        "errByDate = []\n",
        "for row in errByDate_df:\n",
        "  errByDate.append((int(row[\"day\"]),row[\"count\"]))\n",
        "print('404 Errors by day: %s' % errByDate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp_b2lje9pa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST 404 response codes per day (4.4)\n",
        "Test.assertEquals(errByDate, [(1, 243), (3, 303), (4, 346), (5, 234), (6, 372), (7, 532), (8, 381), (9, 279), (10, 314), (11, 263), (12, 195), (13, 216), (14, 287), (15, 326), (16, 258), (17, 269), (18, 255), (19, 207), (20, 312), (21, 305), (22, 288)], 'incorrect errByDate')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ed9yvoL9pa5",
        "colab_type": "text"
      },
      "source": [
        "## 5. Visualizing the 404 Response Codes by Day\n",
        "\n",
        "Using the results from the previous exercise, use `matplotlib` to plot a \"Line\" or \"Bar\" graph of the 404 response codes by day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wvae4499pa5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "daysWithErrors404 = [i[0] for i in errByDate]\n",
        "errors404ByDay = [i[1] for i in errByDate]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kngmN1PE9pa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST Visualizing the 404 Response Codes by Day (4f)\n",
        "Test.assertEquals(daysWithErrors404, [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], 'incorrect daysWithErrors404')\n",
        "Test.assertEquals(errors404ByDay, [243, 303, 346, 234, 372, 532, 381, 279, 314, 263, 195, 216, 287, 326, 258, 269, 255, 207, 312, 305, 288], 'incorrect errors404ByDay')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB40bsQY9pa9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
        "plt.axis([0, max(daysWithErrors404), 0, max(errors404ByDay)])\n",
        "plt.grid(b=True, which='major', axis='y')\n",
        "plt.xlabel('Day')\n",
        "plt.ylabel('404 Errors')\n",
        "plt.plot(daysWithErrors404, errors404ByDay)\n",
        "pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m35V1ps99pbA",
        "colab_type": "text"
      },
      "source": [
        "## 6. Hourly 404 Response Codes\n",
        "\n",
        "Using the RDD / Dataframe `badRecords` you cached in the part (4.1) and by hour of the day and in increasing order, create an RDD / Dataframe containing how many requests had a 404 return code for each hour of the day (midnight starts at 0). Cache the resulting RDD hourRecordsSorted. Assign the result to variable `errHourList`and don't forget to return the result to the driver.\n",
        "\n",
        "_Hint_ : Concerning the Spark SQL approach, multiple methods exist including the use of an UDF similar to exercise (3.4)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0Ib7Ddb9pbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "\n",
        "def date_to_hour(date):\n",
        "    \"\"\"\n",
        "    Extracts the hour from date-time.\n",
        "    date : timestamp with pattern \"yyyy-mm-dd hh:mm:ss\"\n",
        "    returns : day of the month (pattern 'dd')\n",
        "    \"\"\"\n",
        "    str_date = str(date)\n",
        "    s =  re.search(\"\\d\\d\\d\\d-\\d\\d-\\d\\d (\\d\\d):\\d\\d:\\d\\d\",str_date)\n",
        "    return s.group(1)\n",
        "hour_udf = udf(date_to_hour)\n",
        "\n",
        "errHourList_df = badRecords.withColumn(\"hour\",hour_udf(badRecords[\"date_time\"])).groupBy(\"hour\").count().sort(\"hour\").collect()\n",
        "errHourList = []\n",
        "for row in errHourList_df:\n",
        "  errHourList.append((int(row[\"hour\"]),row[\"count\"]))\n",
        "print('Top hours for 404 requests: %s' % errHourList)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvUxwtxW9pbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST Hourly 404 response codes (4h)\n",
        "Test.assertEquals(errHourList, [(0, 175), (1, 171), (2, 422), (3, 272), (4, 102), (5, 95), (6, 93), (7, 122), (8, 199), (9, 185), (10, 329), (11, 263), (12, 438), (13, 397), (14, 318), (15, 347), (16, 373), (17, 330), (18, 268), (19, 269), (20, 270), (21, 241), (22, 234), (23, 272)], 'incorrect errHourList')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60dGw9yP9pbF",
        "colab_type": "text"
      },
      "source": [
        "## 7. Visualizing the 404 Response Codes by Hour\n",
        "\n",
        "Using the results from the previous exercise, use `matplotlib` to plot a \"Line\" or \"Bar\" graph of the 404 response codes by hour.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcYJjOKo9pbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Replace <FILL IN> with appropriate code\n",
        "\n",
        "hoursWithErrors404 = [i[0] for i in errHourList]\n",
        "errors404ByHours = [i[1] for i in errHourList]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP3KZyKQ9pbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TEST Visualizing the 404 Response Codes by Hour (4i)\n",
        "Test.assertEquals(hoursWithErrors404, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], 'incorrect hoursWithErrors404')\n",
        "Test.assertEquals(errors404ByHours, [175, 171, 422, 272, 102, 95, 93, 122, 199, 185, 329, 263, 438, 397, 318, 347, 373, 330, 268, 269, 270, 241, 234, 272], 'incorrect errors404ByHours')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMs-hKJX9pbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(8,4.2), facecolor='white', edgecolor='white')\n",
        "plt.axis([0, max(hoursWithErrors404), 0, max(errors404ByHours)])\n",
        "plt.grid(b=True, which='major', axis='y')\n",
        "plt.xlabel('Hour')\n",
        "plt.ylabel('404 Errors')\n",
        "plt.plot(hoursWithErrors404, errors404ByHours)\n",
        "pass"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}